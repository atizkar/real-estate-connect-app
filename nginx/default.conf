server {
    listen 80; # Listen on port 80 for HTTP connections
    listen [::]:80; # Listen on IPv6 as well

    server_name localhost; # Or your domain name

    # Set the root directory for serving files
    root /usr/share/nginx/html;

    # Set the default index file
    index index.html index.htm;

    # Proxy all requests starting with /llm-api/ to your local LLM Studio instance
    # IMPORTANT: host.docker.internal reliably resolves to the host machine's IP from inside Docker Desktop.
    # LLM Studio should be running on your host machine at port 1234.
    location /llm-api/ {
        proxy_pass http://host.docker.internal:1234/; # Use host.docker.internal here
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;

        # **IMPORTANT: Set proxy timeouts for potentially long LLM responses**
        # Timeout for connecting to the upstream server (LLM Studio)
        proxy_connect_timeout 60s;
        # Timeout for sending a request to the upstream server
        proxy_send_timeout 60s;
        # Timeout for reading a response from the upstream server
        proxy_read_timeout 60s;

        rewrite ^/llm-api/(.*)$ /$1 break;
    }

    # Configure Nginx to serve single-page applications (like React)
    location / {
        try_files $uri $uri/ /index.html;
    }

    # Optional: Serve static assets with long cache headers (for production optimization)
    location ~* \.(js|css|png|jpg|jpeg|gif|ico|svg)$ {
        expires 30d;
        add_header Cache-Control "public, no-transform";
    }

    error_page 500 502 503 504 /50x.html;
    location = /50x.html {
        root /usr/share/nginx/html;
    }
}
